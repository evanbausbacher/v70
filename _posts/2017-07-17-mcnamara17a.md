---
title: Risk Bounds for Transferring Representations With and Without Fine-Tuning
booktitle: Proceedings of the 34th International Conference on Machine Learning
year: '2017'
volume: '70'
series: Proceedings of Machine Learning Research
address: 
month: 0
publisher: PMLR
pdf: http://proceedings.mlr.press/v70/mcnamara17a/mcnamara17a.pdf
url: http://proceedings.mlr.press/v70/mcnamara17a.html
abstract: A popular machine learning strategy is the transfer of a representation
  (i.e. a feature extraction function) learned on a source task to a target task.
  Examples include the re-use of neural network weights or word embeddings. We develop
  sufficient conditions for the success of this approach. If the representation learned
  from the source task is fixed, we identify conditions on how the tasks relate to
  obtain an upper bound on target task risk via a VC dimension-based argument. We
  then consider using the representation from the source task to construct a prior,
  which is fine-tuned using target task data. We give a PAC-Bayes target task risk
  bound in this setting under suitable conditions. We show examples of our bounds
  using feedforward neural networks. Our results motivate a practical approach to
  weight transfer, which we validate with experiments.
layout: inproceedings
id: mcnamara17a
tex_title: Risk Bounds for Transferring Representations With and Without Fine-Tuning
bibtex_author: Daniel McNamara and Maria-Florina Balcan
firstpage: 2373
lastpage: 2381
page: 2373-2381
order: 2373
cycles: false
editor:
- given: Doina
  family: Precup
- given: Yee Whye
  family: Teh
author:
- given: Daniel
  family: McNamara
- given: Maria-Florina
  family: Balcan
date: 2017-07-17
container-title: Proceedings of the 34th International Conference on Machine Learning
genre: inproceedings
issued:
  date-parts:
  - 2017
  - 7
  - 17
extras:
- label: Code for experiments
  link: https://github.com/dpmcna/transfer
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
